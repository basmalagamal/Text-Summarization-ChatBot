{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a38c25",
   "metadata": {},
   "source": [
    "1- call model --->load_model\n",
    "2- Named Entity Recognition\n",
    "3- Google API\n",
    "4- Text Auto-Correction\n",
    "5- Text Summarization\n",
    "6- Buisness Card Scanner\n",
    "7- Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "137cd715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Text_Summarization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Text_Summarization.py\n",
    "import streamlit as st\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from langdetect import detect, LangDetectException\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "# Load Spacy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "st.title('Text Summarization Application')\n",
    "\n",
    "# Text Upload Feature\n",
    "uploaded_file = st.file_uploader(\"Upload a .txt file\", type=[\"txt\"])\n",
    "if uploaded_file is not None:\n",
    "    text = uploaded_file.read().decode(\"utf-8\")\n",
    "    st.text_area(\"File Content\", text)\n",
    "else:\n",
    "    text = st.text_area(\"Please, Enter the text to Summarize...\", height=150)\n",
    "\n",
    "# Summarizer Selection\n",
    "summarizer_type = st.selectbox(\"Choose Summarizer Type\", ('LSA', 'LUHN', 'LexRank', 'TextRank'))\n",
    "\n",
    "# Sentence Count Slider\n",
    "sentence_count = st.slider(\"Number Of Sentences\", 1, 20, 5)\n",
    "\n",
    "# Summarization Function\n",
    "def Summarize_Text(text, summarizer_type='lsa', sentence_count=5, language=\"english\"):\n",
    "    try:\n",
    "        # Set tokenization language dynamically\n",
    "        parser = PlaintextParser.from_string(text, Tokenizer(language))\n",
    "        \n",
    "        if summarizer_type == 'LSA':\n",
    "            summarizer = LsaSummarizer()\n",
    "        elif summarizer_type == 'LUHN':\n",
    "            summarizer = LuhnSummarizer()\n",
    "        elif summarizer_type == 'LexRank':\n",
    "            summarizer = LexRankSummarizer()\n",
    "        elif summarizer_type == 'TextRank':\n",
    "            summarizer = TextRankSummarizer()\n",
    "        \n",
    "        summary = summarizer(parser.document, sentence_count)\n",
    "        return \" \".join(str(sentence) for sentence in summary) # join sentences together\n",
    "    except Exception as e:\n",
    "        return f\"Error in summarization: {str(e)}\"\n",
    "\n",
    "# Summarize Text\n",
    "if st.button('Summarize Text'):\n",
    "    if text:\n",
    "        # Detect language for tokenization\n",
    "        try:\n",
    "            detected_language = detect(text)\n",
    "            st.write(f\"Detected language: {detected_language}\")\n",
    "        except LangDetectException:\n",
    "            detected_language = \"english\"  # fallback to English if detection fails\n",
    "        \n",
    "        # Summarize the text based on detected language\n",
    "        summary = Summarize_Text(text, summarizer_type, sentence_count, detected_language)\n",
    "        st.subheader('Summary')\n",
    "        st.write(summary)\n",
    "    else:\n",
    "        st.write('Please write a text to summarize.')\n",
    "\n",
    "# Language Detection Button\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language\n",
    "    except LangDetectException:\n",
    "        return \"Could not detect language\"\n",
    "\n",
    "if st.button('Detect Language'):\n",
    "    if text:\n",
    "        detected_language = detect_language(text)\n",
    "        st.write(f\"Detected language: {detected_language}\")\n",
    "    else:\n",
    "        st.write('Please enter text to detect language')\n",
    "\n",
    "# Text Auto-Correction Feature\n",
    "def auto_correct_text(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = str(blob.correct())\n",
    "    return corrected_text\n",
    "\n",
    "if st.button('Auto-Correct Text'):\n",
    "    if text:\n",
    "        corrected_text = auto_correct_text(text)\n",
    "        st.subheader('Corrected Text')\n",
    "        st.write(corrected_text)\n",
    "    else:\n",
    "        st.write('Please write or upload text to correct.')\n",
    "\n",
    "# Named Entity Recognition (NER) Feature\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "if st.button('Named Entity Recognition'):\n",
    "    if text:\n",
    "        entities = extract_entities(text)\n",
    "        st.subheader('Named Entities')\n",
    "        for entity, label in entities:\n",
    "            st.write(f\"{entity}: {label}\")\n",
    "    else:\n",
    "        st.write('Please write or upload text for NER.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18261aa-7f50-41c3-b9e3-43e11ef4c071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
